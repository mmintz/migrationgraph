%pip install pandas
%pip install sqlalchemy

import os
import pandas as pd
import psycopg2
import io
import csv
import json
import datetime
import decimal
import sqlalchemy
from sqlalchemy import create_engine, MetaData, inspect, event, text
from collections import deque
from queue import Queue
from collections import deque

import pandas as pd
# Database connection parameters
# Database connection parameters are now retrieved from environment variables
db_params = {
    'host': os.getenv('DB_HOST5', 'partner-prod-temp.postgres.database.azure.com'),
    'dbname': os.getenv('DB_NAME5', 'impactpartners'),
    'user': os.getenv('DB_USER5', 'partnerproductionpsqlmanager@partner-prod-temp'),
    'password': os.getenv('DB_PASSWORD5', 'uH1fSuHsnUnF44GD?5t/6zOQ'),
    'port': int(os.getenv('DB_PORT', '5432'))  # Default port number if not specified
}

def get_table_columns(engine, schema_name, table_name, excluded_columns):
    """
    Retrieve column names for a given table excluding certain columns.
    """
    query = sqlalchemy.text("""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_schema = :schema_name AND table_name = :table_name;
    """)

    with engine.connect() as conn:
        result = conn.execute(query, {'schema_name': schema_name, 'table_name': table_name})
        columns = [row[0] for row in result.fetchall() if row[0] not in excluded_columns]
    
    return columns
def construct_select_clause(schema_name, table_name, columns):
    """
    Construct a SELECT clause for a given table.
    """
    return ', '.join([f'"{schema_name}"."{table_name}"."{col}"' for col in columns])


# Adjusted construct_join_query function to include previous joins and where clause
def construct_join_query(schema_name, starting_table, joined_table, starting_table_column, joined_table_column, key_value, previous_joins, where_clause):
    """
    Construct a full JOIN query with all columns from the joined table, incorporating the schema name, previous joins, and where clause.
    Ensure each table is only joined once.
    """
    # Construct the new join condition
    new_join_condition = f'LEFT JOIN \"{schema_name}\".\"{joined_table}\" ON \"{schema_name}\".\"{starting_table}\".\"{starting_table_column}\" = \"{schema_name}\".\"{joined_table}\".\"{joined_table_column}\"'
    
    # Combine previous joins with the new join condition
    combined_joins = f"{previous_joins} {new_join_condition}"

    # Construct the full join query string
    full_join_query_str = f"SELECT DISTINCT \"{schema_name}\".\"{joined_table}\".* FROM \"{schema_name}\".\"{starting_table}\" {combined_joins} {where_clause}"
    return text(full_join_query_str)
# Establishing the connection
db_url = f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}"
engine = create_engine(db_url)

# Set the schema for this session
schema_name = os.getenv('DB_SCHEMA5', 'partner')  # Retrieve schema name from environment variable

@event.listens_for(engine, "connect")
def set_search_path(dbapi_connection, connection_record):
    cursor = dbapi_connection.cursor()
    cursor.execute(f"SET search_path TO {schema_name};")
    cursor.close()

# Define a default JSON object for fields that are None
default_json = {}  # Update this with a suitable default JSON object
empty_placeholder = "\\N"  # Placeholder for empty fields in .dump file

with engine.connect() as conn:
        print("Successfully connected to the database!")
        conn.execute(text(f"SET search_path TO {schema_name};"))
def export_table_data(query, params, connections, common_dump_file_name, schema_name, table_name, export_format='text', export_path='/app/outputs', log_file_name='query_log.txt',tables_file='tables_log.txt', CHUNK_SIZE=10000):
    if not os.path.exists(export_path):
        os.makedirs(export_path)

    log_file_path = os.path.join(export_path, log_file_name)
    dump_file_path = os.path.join(export_path, f"{common_dump_file_name}.csv")
    tables_file_path = os.path.join(export_path, tables_file)

   
    with open(log_file_path, 'a', encoding='utf-8') as log_file:
        # Convert the query object to a string
        query_str = str(query)
        param_value = repr(params['key_value']) if isinstance(params['key_value'], str) else str(params['key_value'])
        query_str_with_params = query_str.replace(":key_value", param_value)
        # Write the modified query to the log file
        log_file.write(f"Table: {table_name}\n{query_str_with_params}\n")

    with open(tables_file_path, 'a', encoding='utf-8') as tables_file:
        # Write the modified query to the log file
        tables_file.write(f"Table: {table_name}\n")
  
    with engine.connect() as connection:
        try:
            first_chunk = True
            print(f"running for table -> {table_name} chunks with params {params} ----the query {query}")
            for chunk in pd.read_sql_query(query, connection, params=params, chunksize=CHUNK_SIZE):
                if first_chunk:
                    # Write column names and COPY command only for the first chunk
                    column_names = '\t '.join(chunk.columns)
                    header_str = f"{schema_name}.{table_name}\n{column_names}\n"  # Removed the closing parenthesis
                    first_chunk = False
                else:
                    header_str = ''

                chunk.dropna(how='all', inplace=True)

                # Convert the DataFrame to a CSV formatted string
                csv_string = chunk.to_csv(sep='\t', index=False, header=False)

                # Append the header and CSV string to the dump file
                with open(dump_file_path, 'a', encoding='utf-8') as dump_file:
                    dump_file.write(header_str + csv_string)

            # # Write the end-of-data marker
            with open(dump_file_path, 'a', encoding='utf-8') as dump_file:
                dump_file.write("\n")

        except Exception as e:
            print(f"An error occurred: {e}")

    print(f"Data appended to {dump_file_path} with query logged in {log_file_path}")

def generate_truncate_sql(export_path, related_tables):
    truncate_file_path = os.path.join(export_path, "truncate_tables.sql")
    with open(truncate_file_path, 'w', encoding='utf-8') as truncate_file:
        for table in related_tables:
            truncate_file.write(f"TRUNCATE TABLE {schema_name}.{table} restart identity cascade;\n")
    print(f"Truncate script created at {truncate_file_path}")

def get_key_value(table, column, value, connection):
    """
    Fetch the corresponding key value from a table based on a column and its value.
    """
    query = f"SELECT {column} FROM {table} WHERE {column} = %s;"
    with connection.cursor() as cur:
        cur.execute(query, [value])
        result = cur.fetchone()
        return result[0] if result else None

# Usage example
common_dump_file_name = "brands_dump"
# Retrieve script parameters from environment variables

def get_related_tables2(engine, table_name, excluded_tables, excluded_columns):
    related_tables = []
    
    foreign_key_query = """
    SELECT DISTINCT
        tc.table_name AS primary_table, 
        kcu.column_name AS primary_column, 
        ccu.table_name AS foreign_table_name,
        ccu.column_name AS foreign_column
    FROM 
        information_schema.table_constraints AS tc 
        JOIN information_schema.key_column_usage AS kcu 
        ON tc.constraint_name = kcu.constraint_name
        JOIN information_schema.constraint_column_usage AS ccu 
        ON ccu.constraint_name = tc.constraint_name
    WHERE 
        tc.constraint_type = 'FOREIGN KEY' AND 
        (tc.table_name = :current_table OR ccu.table_name = :current_table);
    """
    with open("/app/outputs/get_related_tables_output.txt", "a") as file:
        with engine.connect() as conn:
            result = conn.execute(sqlalchemy.text(foreign_key_query), {'current_table': table_name})
            rows = result.fetchall()

            for row in rows:
                file.write(f"Row: {row}\n")  # Log each row
                primary_table, primary_column, foreign_table_name, foreign_column = row
                if primary_column in excluded_columns or foreign_column in excluded_columns:
                    continue

                if primary_table == table_name and foreign_table_name not in excluded_tables:
                    related_tables.append((foreign_table_name, primary_column, foreign_column))
                elif foreign_table_name == table_name and primary_table not in excluded_tables:
                    related_tables.append((primary_table, foreign_column, primary_column))
        file.write(f"RETURNING : {related_tables}\n")  # Log each row
        return related_tables #dame na filaoume tse to column...tse sto traverse na ta filai swsta sto path.
                                                #kai meta na kamoume ta left joins swsta san kinoumaste sto path sto combine

    #return list(set(related_tables))

def bfs_traverse(engine,relationships_dict, start_table, schema_name, key_column, key_value, bfs_output_file, excluded_tables, excluded_columns):
    if excluded_tables is None:
        excluded_tables = set()
    if excluded_columns is None:
        excluded_columns = set()

    queue = deque([(start_table, [])])
    paths = {}
    visited_paths = set()  # Track visited paths to avoid cycles

    with open(bfs_output_file, 'w') as file:
        while queue:
            current_table, path = queue.popleft()

            # Create a new path to include the current table
            new_path = path + [current_table]

            # Skip if the path has been visited
            path_tuple = tuple(new_path)
            if path_tuple in visited_paths:
                continue

            # Add the new path to visited paths and paths dictionary
            visited_paths.add(path_tuple)
            paths.setdefault(current_table, []).append(new_path)

            # Process related tables
            for related_table_info in relationships_dict.get(current_table, []):
                related_table, primary_column, foreign_column = related_table_info

                # Check for exclusions
                if related_table in excluded_tables or primary_column in excluded_columns or foreign_column in excluded_columns:
                    continue

                # Build the join condition
                join_condition = f'LEFT JOIN \"{schema_name}\".\"{related_table}\" ON \"{schema_name}\".\"{current_table}\".\"{primary_column}\" = \"{schema_name}\".\"{related_table}\".\"{foreign_column}\"'
                
                # Append related table and join condition to the path and queue
                queue.append((related_table, new_path + [(related_table, join_condition)]))

            # Log the path for each table
            file.write(f"Table: {current_table}, Path: {new_path}\n")
        
        file.write(f"RETURNING PATHS : {paths}\n")
    
    return paths

def combine_union_queries(engine,relationships_dict, start_table, start_table_key_column, key_value, schema_name, bfs_output_file, excluded_tables=None, excluded_columns=None):
    paths = bfs_traverse(engine,relationships_dict, start_table, schema_name, start_table_key_column, key_value, bfs_output_file, excluded_tables, excluded_columns)
    with open("/app/outputs/combine_union_queries_output.txt", "w") as file:
       for table, table_paths in paths.items():
            table_queries = []

            for path in table_paths:
                join_part = " ".join(path)
                query = f"SELECT DISTINCT \"{schema_name}\".\"{table}\".* FROM \"{schema_name}\".\"{start_table}\" {join_part} WHERE \"{schema_name}\".\"{start_table}\".\"{start_table_key_column}\" = '{key_value}' AND \"{schema_name}\".\"{table}\".\"id\" IS NOT NULL"
                table_queries.append(query)

            final_query = " UNION ".join(table_queries) if len(table_queries) > 1 else table_queries[0]

            # Execute the query and export the data
            with engine.connect() as conn:
                export_table_data(final_query, {'key_value': key_value}, conn, common_dump_file_name, schema_name, table, export_format='text')
            
            # Logging
            file.write(f"Table: {table}, Query: {final_query}\n")
            #print(f"Table: {table}, Query: {final_query}\n")

db_params = {
    'host': os.getenv('DB_HOST', 'default_host'),
    'dbname': os.getenv('DB_NAME', 'default_db_name'),
    'user': os.getenv('DB_USER', 'default_user'),
    'password': os.getenv('DB_PASSWORD', 'default_password'),
    'port': int(os.getenv('DB_PORT', '5432'))  # Default port number if not specified
}

def extract_relationships():
    relationships = []

    foreign_key_query = """
    SELECT DISTINCT
        tc.table_name AS primary_table, 
        kcu.column_name AS primary_column, 
        ccu.table_name AS foreign_table_name,
        ccu.column_name AS foreign_column
    FROM 
        information_schema.table_constraints AS tc 
        JOIN information_schema.key_column_usage AS kcu 
        ON tc.constraint_name = kcu.constraint_name
        JOIN information_schema.constraint_column_usage AS ccu 
        ON ccu.constraint_name = tc.constraint_name
    WHERE 
        tc.constraint_type = 'FOREIGN KEY';
    """

    with engine.connect() as conn:
        result = conn.execute(sqlalchemy.text(foreign_key_query))
        for row in result:
            primary_table = row[0]  # primary_table
            primary_column = row[1]  # primary_column
            foreign_table = row[2]  # foreign_table_name
            foreign_column = row[3]  # foreign_column
            relationships.append((primary_table, primary_column, foreign_table, foreign_column))

    return relationships

def generate_relationships_dict(relationships):
    relationships_dict = {}
    for primary_table, primary_column, foreign_table, foreign_column in relationships:
        if primary_table not in relationships_dict:
            relationships_dict[primary_table] = []
        relationships_dict[primary_table].append((foreign_table, primary_column, foreign_column))

    return relationships_dict
def generate_sql_queries(relationships_dict, schema_name):
    starting_table = os.getenv('STARTING_TABLE', 'tenant')  # Default to 'tenant'
    starting_key_column = os.getenv('STARTING_KEY_COLUMN', 'id')  # Default to 'id'

    # For 'key_value', handle different data types (string/int)
    key_value_str = os.getenv('KEY_VALUE', 'default_value_for_key_value')
    try:
        key_value = int(key_value_str)
    except ValueError:
        key_value = key_value_str  # Use the string value if it's not an integer

    excluded_tables_str = os.getenv('EXCLUDED_TABLES', '')
    excluded_tables = set(excluded_tables_str.split(',')) if excluded_tables_str else set()
    excluded_columns = {'modified_by_id', 'created_by_id', 'symbol_id'}

    def construct_query(table, path):
        join_parts = []
        current_table = starting_table
        for rel_table, primary_col, foreign_col in path:
            if rel_table in excluded_tables or foreign_col in excluded_columns or primary_col in excluded_columns:
                continue
            join_condition = f'LEFT JOIN "{schema_name}"."{rel_table}" ON "{schema_name}"."{current_table}"."{primary_col}" = "{schema_name}"."{rel_table}"."{foreign_col}"'
            join_parts.append(join_condition)
            current_table = rel_table
        return f'SELECT DISTINCT "{schema_name}"."{table}".* FROM "{schema_name}"."{starting_table}" ' + ' '.join(join_parts) + f' WHERE "{schema_name}"."{starting_table}"."{starting_key_column}" = \'{key_value}\' AND "{schema_name}"."{table}"."id" IS NOT NULL'

    def find_paths(start_table, target_table, visited, path):
        if start_table == target_table:
            yield path
            return
        if start_table in visited:
            return
        visited.add(start_table)
        for related_table_info in relationships_dict.get(start_table, []):
            for next_path in find_paths(related_table_info[0], target_table, visited.copy(), path + [related_table_info]):
                yield next_path

    queries = {}
    for table in relationships_dict.keys():
        if table not in excluded_tables:
            paths = list(find_paths(starting_table, table, set(), []))
            table_queries = [construct_query(table, path) for path in paths]
            queries[table] = ' UNION '.join(table_queries) if len(table_queries) > 1 else (table_queries[0] if table_queries else None)

    return queries



# Extract relationships from the database
relationships = extract_relationships()

# Generate the relationships dictionary
relationships_dict = generate_relationships_dict(relationships)
# Printing relationships_dict for demonstration
with open("/app/outputs/relationships.txt", "w") as file:
    for table, connected_tables in relationships_dict.items():
        file.write(f"{table}: {connected_tables}\n")
starting_table = os.getenv('STARTING_TABLE', 'default_value_for_starting_table')
starting_key_column = os.getenv('STARTING_KEY_COLUMN', 'default_value_for_starting_key_column')

# For 'key_value', handle different data types (string/int)
key_value_str = os.getenv('KEY_VALUE', 'default_value_for_key_value')
try:
    key_value = int(key_value_str)
except ValueError:
    key_value = key_value_str  # Use the string value if it's not an integer

# Split the excluded tables into a list if they are provided as a comma-separated string
excluded_tables_str = os.getenv('EXCLUDED_TABLES', '')
excluded_tables = excluded_tables_str.split(',') if excluded_tables_str else []
excluded_columns = {'modified_by_id', 'created_by_id', 'symbol_id'}
# Global chunk size for data processing
CHUNK_SIZE = 10000  # You can adjust this value as needed
queries = generate_sql_queries(relationships_dict, schema_name)
#combine_union_queries(engine,relationships_dict, starting_table, starting_key_column, key_value, schema_name, '/app/outputs/bfs_output.txt', excluded_tables, excluded_columns)
with open("/app/outputs/queires.txt", "w") as file:
    for table, query in queries.items():
        if query:
            file.write(f"Table: {table}, Query: {query}\n")
# Specify the file path where you want to save the output
#output_file_path = '/app/exports/related_tables_output.txt'



