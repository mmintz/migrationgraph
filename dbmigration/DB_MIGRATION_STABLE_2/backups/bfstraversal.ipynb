{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python38\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mario\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python38\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python38\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\python38\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mario\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlalchemy in c:\\python38\\lib\\site-packages (2.0.23)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\mario\\appdata\\roaming\\python\\python38\\site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python38\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully connected to the database!\n",
      "Hierarchy starting from 'default_value_for_starting_table', for key 'default_value_for_starting_key_column': default_value_for_key_value\n",
      "running for table -> default_value_for_starting_table chunks with params {'key_value': 'default_value_for_key_value'} ----the query SELECT DISTINCT * FROM \"partner\".\"default_value_for_starting_table\"  WHERE \"partner\".\"default_value_for_starting_table\".\"default_value_for_starting_key_column\" = :key_value\n",
      "An error occurred: (psycopg2.errors.UndefinedTable) relation \"partner.default_value_for_starting_table\" does not exist\n",
      "LINE 1: SELECT DISTINCT * FROM \"partner\".\"default_value_for_starting...\n",
      "                               ^\n",
      "\n",
      "[SQL: SELECT DISTINCT * FROM \"partner\".\"default_value_for_starting_table\"  WHERE \"partner\".\"default_value_for_starting_table\".\"default_value_for_starting_key_column\" = %(key_value)s]\n",
      "[parameters: {'key_value': 'default_value_for_key_value'}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n",
      "Data appended to /app/outputs\\brands_dump.csv with query logged in /app/outputs\\query_log.txt\n",
      "Current to process table  TO default_value_for_starting_table previous JOINS joins: \n",
      "Current to process table  TO default_value_for_starting_table previous RESULTS FETCHED: []\n",
      "Related tables: {'default_value_for_starting_table'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/exports/related_tables_output.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mario\\Desktop\\LaMin\\Projects\\ProjectX\\dbmigration\\DB_MIGRATION_STABLE_2\\bfstraversal.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=303'>304</a>\u001b[0m output_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/app/exports/related_tables_output.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=305'>306</a>\u001b[0m \u001b[39m# Writing the related tables to a file\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=306'>307</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(output_file_path, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=307'>308</a>\u001b[0m     \u001b[39mfor\u001b[39;00m table \u001b[39min\u001b[39;00m related_tables:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=308'>309</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtable\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/app/exports/related_tables_output.txt'"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install sqlalchemy\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import io\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import decimal\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, MetaData, inspect, event, text\n",
    "from collections import deque\n",
    "from queue import Queue\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "# Database connection parameters\n",
    "# Database connection parameters are now retrieved from environment variables\n",
    "db_params = {\n",
    "    'host': os.getenv('DB_HOST5', 'partner-prod-temp.postgres.database.azure.com'),\n",
    "    'dbname': os.getenv('DB_NAME5', 'impactpartners'),\n",
    "    'user': os.getenv('DB_USER5', 'partnerproductionpsqlmanager@partner-prod-temp'),\n",
    "    'password': os.getenv('DB_PASSWORD5', 'uH1fSuHsnUnF44GD?5t/6zOQ'),\n",
    "    'port': int(os.getenv('DB_PORT', '5432'))  # Default port number if not specified\n",
    "}\n",
    "\n",
    "def get_table_columns(engine, schema_name, table_name, excluded_columns):\n",
    "    \"\"\"\n",
    "    Retrieve column names for a given table excluding certain columns.\n",
    "    \"\"\"\n",
    "    query = sqlalchemy.text(\"\"\"\n",
    "        SELECT column_name \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_schema = :schema_name AND table_name = :table_name;\n",
    "    \"\"\")\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {'schema_name': schema_name, 'table_name': table_name})\n",
    "        columns = [row[0] for row in result.fetchall() if row[0] not in excluded_columns]\n",
    "    \n",
    "    return columns\n",
    "def construct_select_clause(schema_name, table_name, columns):\n",
    "    \"\"\"\n",
    "    Construct a SELECT clause for a given table.\n",
    "    \"\"\"\n",
    "    return ', '.join([f'\"{schema_name}\".\"{table_name}\".\"{col}\"' for col in columns])\n",
    "\n",
    "# Adjusted construct_join_query function\n",
    "# Adjusted construct_join_query function\n",
    "# Adjusted construct_join_query function to include previous joins and where clause\n",
    "def construct_join_query(schema_name, starting_table, joined_table, starting_table_column, joined_table_column, key_value, previous_joins, where_clause):\n",
    "    \"\"\"\n",
    "    Construct a full JOIN query with all columns from the joined table, incorporating the schema name, previous joins, and where clause.\n",
    "    Ensure each table is only joined once.\n",
    "    \"\"\"\n",
    "    # Construct the new join condition\n",
    "    new_join_condition = f'LEFT JOIN \\\"{schema_name}\\\".\\\"{joined_table}\\\" ON \\\"{schema_name}\\\".\\\"{starting_table}\\\".\\\"{starting_table_column}\\\" = \\\"{schema_name}\\\".\\\"{joined_table}\\\".\\\"{joined_table_column}\\\"'\n",
    "    \n",
    "    # Combine previous joins with the new join condition\n",
    "    combined_joins = f\"{previous_joins} {new_join_condition}\"\n",
    "\n",
    "    # Construct the full join query string\n",
    "    full_join_query_str = f\"SELECT DISTINCT \\\"{schema_name}\\\".\\\"{joined_table}\\\".* FROM \\\"{schema_name}\\\".\\\"{starting_table}\\\" {combined_joins} {where_clause}\"\n",
    "    return text(full_join_query_str)\n",
    "# Establishing the connection\n",
    "db_url = f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "# Set the schema for this session\n",
    "schema_name = os.getenv('DB_SCHEMA5', 'partner')  # Retrieve schema name from environment variable\n",
    "\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def set_search_path(dbapi_connection, connection_record):\n",
    "    cursor = dbapi_connection.cursor()\n",
    "    cursor.execute(f\"SET search_path TO {schema_name};\")\n",
    "    cursor.close()\n",
    "# with conn.cursor() as cur:\n",
    "#     cur.execute(f\"SET search_path TO {schema_name};\")\n",
    "\n",
    "# Define a default JSON object for fields that are None\n",
    "default_json = {}  # Update this with a suitable default JSON object\n",
    "empty_placeholder = \"\\\\N\"  # Placeholder for empty fields in .dump file\n",
    "\n",
    "with engine.connect() as conn:\n",
    "        print(\"Successfully connected to the database!\")\n",
    "        conn.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "def export_table_data(query, params, connections, common_dump_file_name, schema_name, table_name, export_format='text', export_path='/app/outputs', log_file_name='query_log.txt',tables_file='tables_log.txt', CHUNK_SIZE=10000):\n",
    "    if not os.path.exists(export_path):\n",
    "        os.makedirs(export_path)\n",
    "\n",
    "    log_file_path = os.path.join(export_path, log_file_name)\n",
    "    dump_file_path = os.path.join(export_path, f\"{common_dump_file_name}.csv\")\n",
    "    tables_file_path = os.path.join(export_path, tables_file)\n",
    "\n",
    "    # with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "    #     # Convert the query object to a string\n",
    "    #     query_str = str(query)\n",
    "\n",
    "    #     # Manually replace the parameter placeholder with its value\n",
    "    #     # Make sure to convert the parameter to a string if it's not already one\n",
    "    #     # Use repr() to handle string parameters correctly by adding quotes\n",
    "    #     if isinstance(params['key_value'], str):\n",
    "    #         param_value = repr(params['key_value'])\n",
    "    #     else:\n",
    "    #         param_value = str(params['key_value'])\n",
    "\n",
    "    #     query_str_with_params = query_str.replace(\":key_value\", param_value)\n",
    "\n",
    "    #     # Write the modified query to the log file\n",
    "    #     #log_file.write(f\"{query_str_with_params}\\n\")\n",
    "    #     log_file.write(f\"Table: {table_name}\\n{query_str_with_params}\\n\")\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        # Convert the query object to a string\n",
    "        query_str = str(query)\n",
    "\n",
    "        # Manually replace the parameter placeholder with its value\n",
    "        # Use repr() to handle string parameters correctly by adding quotes\n",
    "        param_value = repr(params['key_value']) if isinstance(params['key_value'], str) else str(params['key_value'])\n",
    "        query_str_with_params = query_str.replace(\":key_value\", param_value)\n",
    "\n",
    "        # Write the modified query to the log file\n",
    "        log_file.write(f\"Table: {table_name}\\n{query_str_with_params}\\n\")\n",
    "\n",
    "    with open(tables_file_path, 'a', encoding='utf-8') as tables_file:\n",
    "        # Write the modified query to the log file\n",
    "        #log_file.write(f\"{query_str_with_params}\\n\")\n",
    "        tables_file.write(f\"Table: {table_name}\\n\")\n",
    "  \n",
    "    with engine.connect() as connection:\n",
    "        try:\n",
    "            first_chunk = True\n",
    "            print(f\"running for table -> {table_name} chunks with params {params} ----the query {query}\")\n",
    "            for chunk in pd.read_sql_query(query, connection, params=params, chunksize=CHUNK_SIZE):\n",
    "                if first_chunk:\n",
    "                    # Write column names and COPY command only for the first chunk\n",
    "                    column_names = '\\t '.join(chunk.columns)\n",
    "                    header_str = f\"{schema_name}.{table_name}\\n{column_names}\\n\"  # Removed the closing parenthesis\n",
    "                    first_chunk = False\n",
    "                else:\n",
    "                    header_str = ''\n",
    "\n",
    "                chunk.dropna(how='all', inplace=True)\n",
    "\n",
    "                # Convert the DataFrame to a CSV formatted string\n",
    "                csv_string = chunk.to_csv(sep='\\t', index=False, header=False)\n",
    "\n",
    "                # Append the header and CSV string to the dump file\n",
    "                with open(dump_file_path, 'a', encoding='utf-8') as dump_file:\n",
    "                    dump_file.write(header_str + csv_string)\n",
    "\n",
    "            # # Write the end-of-data marker\n",
    "            with open(dump_file_path, 'a', encoding='utf-8') as dump_file:\n",
    "                dump_file.write(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(f\"Data appended to {dump_file_path} with query logged in {log_file_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "def generate_truncate_sql(export_path, related_tables):\n",
    "    truncate_file_path = os.path.join(export_path, \"truncate_tables.sql\")\n",
    "    with open(truncate_file_path, 'w', encoding='utf-8') as truncate_file:\n",
    "        for table in related_tables:\n",
    "            truncate_file.write(f\"TRUNCATE TABLE {schema_name}.{table} restart identity cascade;\\n\")\n",
    "    print(f\"Truncate script created at {truncate_file_path}\")\n",
    "\n",
    "\n",
    "def get_key_value(table, column, value, connection):\n",
    "    \"\"\"\n",
    "    Fetch the corresponding key value from a table based on a column and its value.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT {column} FROM {table} WHERE {column} = %s;\"\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(query, [value])\n",
    "        result = cur.fetchone()\n",
    "        return result[0] if result else None\n",
    "\n",
    "\n",
    "def find_related_tables_bfs(engine, starting_table, start_table_key_column, key_value, common_dump_file_name, schema_name, excluded_tables=None, excluded_columns=None):\n",
    "    if excluded_tables is None:\n",
    "        excluded_tables = set()\n",
    "    if excluded_columns is None:\n",
    "        excluded_columns = {'modified_by_id', 'created_by_id','symbol_id'}\n",
    "\n",
    "    queue = deque([(starting_table, '')])\n",
    "    visited = set()\n",
    "\n",
    "    while queue:\n",
    "        current_table, previous_joins = queue.popleft()\n",
    "\n",
    "        where_clause = f'WHERE \"{schema_name}\".\"{starting_table}\".\"{start_table_key_column}\" = :key_value'\n",
    "       \n",
    "        if current_table == starting_table:\n",
    "            visited.add(current_table)\n",
    "            initial_query_str = f'SELECT DISTINCT * FROM \"{schema_name}\".\"{current_table}\"  WHERE \"{schema_name}\".\"{starting_table}\".\"{start_table_key_column}\" = :key_value'\n",
    "            # Convert the string query to a SQLAlchemy text object\n",
    "            initial_query = text(initial_query_str)\n",
    "            with engine.connect() as conn:\n",
    "                export_table_data(initial_query, {'key_value': key_value}, conn, common_dump_file_name, schema_name, current_table, export_format='text')\n",
    "\n",
    "        foreign_key_query = \"\"\"\n",
    "        SELECT DISTINCT\n",
    "            tc.table_name AS primary_table, \n",
    "            kcu.column_name AS primary_column, \n",
    "            ccu.table_name AS foreign_table_name,\n",
    "            ccu.column_name AS foreign_column\n",
    "        FROM \n",
    "            information_schema.table_constraints AS tc \n",
    "            JOIN information_schema.key_column_usage AS kcu \n",
    "            ON tc.constraint_name = kcu.constraint_name\n",
    "            JOIN information_schema.constraint_column_usage AS ccu \n",
    "            ON ccu.constraint_name = tc.constraint_name\n",
    "        WHERE \n",
    "            tc.constraint_type = 'FOREIGN KEY' AND \n",
    "            (tc.table_name = :current_table OR ccu.table_name = :current_table);\n",
    "        \"\"\"\n",
    "        print(f\"Current to process table  TO {current_table} previous JOINS joins: {previous_joins}\")\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(sqlalchemy.text(foreign_key_query), {'current_table': current_table})\n",
    "            rows = result.fetchall()\n",
    "            print(f\"Current to process table  TO {current_table} previous RESULTS FETCHED: {rows}\")\n",
    "            for row in rows:\n",
    "                primary_table, primary_column, foreign_table_name, foreign_column = row\n",
    "\n",
    "                \n",
    "                print(f\"Current to process table  TO {current_table} prIMARY: {primary_table}, COL {primary_column} foreign {foreign_table_name} col {foreign_column}\")\n",
    "                # Skip processing for excluded columns\n",
    "                if primary_column in excluded_columns or foreign_column in excluded_columns:\n",
    "                    print(f\"SKIIIIIIIIIIIIIIIIIIIIIP\")\n",
    "                    continue\n",
    "                if primary_table != current_table: \n",
    "                # Usage of construct_join_query in your code\n",
    "                    if primary_table not in visited and primary_table not in excluded_tables:\n",
    "                        new_join_condition = f'LEFT JOIN \\\"{schema_name}\\\".\\\"{primary_table}\\\" ON \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\".\\\"{foreign_column}\\\" = \\\"{schema_name}\\\".\\\"{primary_table}\\\".\\\"{primary_column}\\\"'\n",
    "                        #full_join_query = construct_join_query(schema_name, foreign_table_name, primary_table, foreign_column, primary_column, key_value, previous_joins, where_clause)\n",
    "                        full_join_query =  f\"SELECT DISTINCT \\\"{schema_name}\\\".\\\"{primary_table}\\\".* FROM \\\"{schema_name}\\\".\\\"{starting_table}\\\"  {previous_joins}  {new_join_condition} {where_clause} AND \\\"{schema_name}\\\".\\\"{primary_table}\\\".\\\"{primary_column}\\\" IS NOT NULL\"\n",
    "                        \n",
    "                        export_table_data(full_join_query, {'key_value': key_value}, conn, common_dump_file_name, schema_name, primary_table, export_format='text')\n",
    "                        next_joins = f\"{previous_joins} {new_join_condition}\"\n",
    "                        print(f\"APPENDING TO {primary_table} next joins: {next_joins}\")\n",
    "                        queue.append((primary_table, next_joins))\n",
    "                        visited.add(primary_table)\n",
    "                        \n",
    "                elif foreign_table_name != current_table:\n",
    "                    if foreign_table_name not in visited and foreign_table_name not in excluded_tables:\n",
    "                        new_join_condition = f'LEFT JOIN \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\" ON \\\"{schema_name}\\\".\\\"{primary_table}\\\".\\\"{primary_column}\\\" = \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\".\\\"{foreign_column}\\\"'\n",
    "                        #full_join_query = construct_join_query(schema_name, primary_table, foreign_table_name, primary_column, foreign_column, key_value, previous_joins, where_clause)\n",
    "                        full_join_query =  f\"SELECT DISTINCT \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\".* FROM \\\"{schema_name}\\\".\\\"{starting_table}\\\" {previous_joins} {new_join_condition} {where_clause} AND \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\".\\\"{foreign_column}\\\" IS NOT NULL\"\n",
    "                        export_table_data(full_join_query, {'key_value': key_value}, conn, common_dump_file_name, schema_name, foreign_table_name, export_format='text')\n",
    "                        next_joins = f\"{previous_joins} {new_join_condition}\"\n",
    "                        print(f\"APPENDING TO {foreign_table_name} next joins: {next_joins}\")\n",
    "                        queue.append((foreign_table_name, next_joins))\n",
    "                        visited.add(foreign_table_name)\n",
    "                    # Process each foreign key relationship\n",
    "                    # Your existing logic for handling relationships goes here\n",
    "\n",
    "    #return visited  # Optionally return the visited set for further analysis\n",
    "    return visited  # Optionally return the visited set for further analysis\n",
    "\n",
    "# Usage example\n",
    "common_dump_file_name = \"brands_dump\"\n",
    "# Retrieve script parameters from environment variables\n",
    "\n",
    "db_params = {\n",
    "    'host': os.getenv('DB_HOST', 'default_host'),\n",
    "    'dbname': os.getenv('DB_NAME', 'default_db_name'),\n",
    "    'user': os.getenv('DB_USER', 'default_user'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'default_password'),\n",
    "    'port': int(os.getenv('DB_PORT', '5432'))  # Default port number if not specified\n",
    "}\n",
    "\n",
    "\n",
    "starting_table = os.getenv('STARTING_TABLE', 'default_value_for_starting_table')\n",
    "starting_key_column = os.getenv('STARTING_KEY_COLUMN', 'default_value_for_starting_key_column')\n",
    "\n",
    "# For 'key_value', handle different data types (string/int)\n",
    "key_value_str = os.getenv('KEY_VALUE', 'default_value_for_key_value')\n",
    "try:\n",
    "    key_value = int(key_value_str)\n",
    "except ValueError:\n",
    "    key_value = key_value_str  # Use the string value if it's not an integer\n",
    "\n",
    "# Split the excluded tables into a list if they are provided as a comma-separated string\n",
    "excluded_tables_str = os.getenv('EXCLUDED_TABLES', '')\n",
    "excluded_tables = excluded_tables_str.split(',') if excluded_tables_str else []\n",
    "\n",
    "# Global chunk size for data processing\n",
    "CHUNK_SIZE = 10000  # You can adjust this value as needed\n",
    "print(\"Hierarchy starting from '{}', for key '{}': {}\".format(starting_table, starting_key_column, key_value))\n",
    "related_tables = find_related_tables_bfs(engine,starting_table, starting_key_column, key_value, common_dump_file_name, schema_name, excluded_tables=excluded_tables)\n",
    "#def find_related_tables_bfs(engine, starting_table, start_table_key_column, key_value, common_dump_file_name, schema_name, excluded_tables=None, excluded_columns=None):\n",
    "  \n",
    "print(f\"Related tables: {related_tables}\")\n",
    "\n",
    "\n",
    "# Specify the file path where you want to save the output\n",
    "output_file_path = '/app/exports/related_tables_output.txt'\n",
    "\n",
    "# Writing the related tables to a file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    for table in related_tables:\n",
    "        file.write(f\"{table}\\n\")\n",
    "\n",
    "print(f\"Related tables have been written to {output_file_path}\")\n",
    "\n",
    "generate_truncate_sql('/app/exports/', related_tables)\n",
    "#engine.dispose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
