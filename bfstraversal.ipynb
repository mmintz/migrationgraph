{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python38\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mario\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python38\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\python38\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\python38\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mario\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sqlalchemy in c:\\python38\\lib\\site-packages (2.0.23)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\mario\\appdata\\roaming\\python\\python38\\site-packages (from sqlalchemy) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python38\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Successfully connected to the database!\n",
      "Hierarchy starting from 'default_value_for_starting_table', for key 'default_value_for_starting_key_column': default_value_for_key_value\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bfs_traverse() missing 6 required positional arguments: 'schema_name', 'key_column', 'key_value', 'bfs_output_file', 'excluded_tables', and 'excluded_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mario\\Desktop\\LaMin\\Projects\\ProjectX\\dbmigration\\DB_MIGRATION_STABLE_2\\bfstraversal.ipynb Cell 1\u001b[0m line \u001b[0;36m5\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=549'>550</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mHierarchy starting from \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, for key \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(starting_table, starting_key_column, key_value))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=550'>551</a>\u001b[0m \u001b[39m#related_tables = find_related_tables_bfs_enhanced2(engine,starting_table, starting_key_column, key_value, common_dump_file_name, schema_name,'/app/outputs/debuging.txt', excluded_tables=excluded_tables)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=551'>552</a>\u001b[0m \u001b[39m#def find_related_tables_bfs(engine, starting_table, start_table_key_column, key_value, common_dump_file_name, schema_name, excluded_tables=None, excluded_columns=None):\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=552'>553</a>\u001b[0m combine_union_queries(engine, starting_table, schema_name, starting_key_column)  \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=553'>554</a>\u001b[0m \u001b[39m#print(f\"Related tables: {related_tables}\")\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=554'>555</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=555'>556</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=556'>557</a>\u001b[0m \u001b[39m# Specify the file path where you want to save the output\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=557'>558</a>\u001b[0m output_file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/app/exports/related_tables_output.txt\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\mario\\Desktop\\LaMin\\Projects\\ProjectX\\dbmigration\\DB_MIGRATION_STABLE_2\\bfstraversal.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=426'>427</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_union_queries\u001b[39m(engine, start_table, schema_name, key_column):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=427'>428</a>\u001b[0m     paths \u001b[39m=\u001b[39m bfs_traverse(engine, start_table)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=429'>430</a>\u001b[0m     \u001b[39mfor\u001b[39;00m table, table_paths \u001b[39min\u001b[39;00m paths\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/mario/Desktop/LaMin/Projects/ProjectX/dbmigration/DB_MIGRATION_STABLE_2/bfstraversal.ipynb#W0sZmlsZQ%3D%3D?line=430'>431</a>\u001b[0m         \u001b[39mif\u001b[39;00m table \u001b[39m==\u001b[39m start_table:\n",
      "\u001b[1;31mTypeError\u001b[0m: bfs_traverse() missing 6 required positional arguments: 'schema_name', 'key_column', 'key_value', 'bfs_output_file', 'excluded_tables', and 'excluded_columns'"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install sqlalchemy\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import io\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import decimal\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, MetaData, inspect, event, text\n",
    "from collections import deque\n",
    "from queue import Queue\n",
    "from collections import deque\n",
    "\n",
    "import pandas as pd\n",
    "# Database connection parameters\n",
    "# Database connection parameters are now retrieved from environment variables\n",
    "db_params = {\n",
    "    'host': os.getenv('DB_HOST5', 'partner-prod-temp.postgres.database.azure.com'),\n",
    "    'dbname': os.getenv('DB_NAME5', 'impactpartners'),\n",
    "    'user': os.getenv('DB_USER5', 'partnerproductionpsqlmanager@partner-prod-temp'),\n",
    "    'password': os.getenv('DB_PASSWORD5', 'uH1fSuHsnUnF44GD?5t/6zOQ'),\n",
    "    'port': int(os.getenv('DB_PORT', '5432'))  # Default port number if not specified\n",
    "}\n",
    "\n",
    "def get_table_columns(engine, schema_name, table_name, excluded_columns):\n",
    "    \"\"\"\n",
    "    Retrieve column names for a given table excluding certain columns.\n",
    "    \"\"\"\n",
    "    query = sqlalchemy.text(\"\"\"\n",
    "        SELECT column_name \n",
    "        FROM information_schema.columns \n",
    "        WHERE table_schema = :schema_name AND table_name = :table_name;\n",
    "    \"\"\")\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(query, {'schema_name': schema_name, 'table_name': table_name})\n",
    "        columns = [row[0] for row in result.fetchall() if row[0] not in excluded_columns]\n",
    "    \n",
    "    return columns\n",
    "def construct_select_clause(schema_name, table_name, columns):\n",
    "    \"\"\"\n",
    "    Construct a SELECT clause for a given table.\n",
    "    \"\"\"\n",
    "    return ', '.join([f'\"{schema_name}\".\"{table_name}\".\"{col}\"' for col in columns])\n",
    "\n",
    "# Adjusted construct_join_query function\n",
    "# Adjusted construct_join_query function\n",
    "# Adjusted construct_join_query function to include previous joins and where clause\n",
    "def construct_join_query(schema_name, starting_table, joined_table, starting_table_column, joined_table_column, key_value, previous_joins, where_clause):\n",
    "    \"\"\"\n",
    "    Construct a full JOIN query with all columns from the joined table, incorporating the schema name, previous joins, and where clause.\n",
    "    Ensure each table is only joined once.\n",
    "    \"\"\"\n",
    "    # Construct the new join condition\n",
    "    new_join_condition = f'LEFT JOIN \\\"{schema_name}\\\".\\\"{joined_table}\\\" ON \\\"{schema_name}\\\".\\\"{starting_table}\\\".\\\"{starting_table_column}\\\" = \\\"{schema_name}\\\".\\\"{joined_table}\\\".\\\"{joined_table_column}\\\"'\n",
    "    \n",
    "    # Combine previous joins with the new join condition\n",
    "    combined_joins = f\"{previous_joins} {new_join_condition}\"\n",
    "\n",
    "    # Construct the full join query string\n",
    "    full_join_query_str = f\"SELECT DISTINCT \\\"{schema_name}\\\".\\\"{joined_table}\\\".* FROM \\\"{schema_name}\\\".\\\"{starting_table}\\\" {combined_joins} {where_clause}\"\n",
    "    return text(full_join_query_str)\n",
    "# Establishing the connection\n",
    "db_url = f\"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "\n",
    "# Set the schema for this session\n",
    "schema_name = os.getenv('DB_SCHEMA5', 'partner')  # Retrieve schema name from environment variable\n",
    "\n",
    "@event.listens_for(engine, \"connect\")\n",
    "def set_search_path(dbapi_connection, connection_record):\n",
    "    cursor = dbapi_connection.cursor()\n",
    "    cursor.execute(f\"SET search_path TO {schema_name};\")\n",
    "    cursor.close()\n",
    "# with conn.cursor() as cur:\n",
    "#     cur.execute(f\"SET search_path TO {schema_name};\")\n",
    "\n",
    "# Define a default JSON object for fields that are None\n",
    "default_json = {}  # Update this with a suitable default JSON object\n",
    "empty_placeholder = \"\\\\N\"  # Placeholder for empty fields in .dump file\n",
    "\n",
    "with engine.connect() as conn:\n",
    "        print(\"Successfully connected to the database!\")\n",
    "        conn.execute(text(f\"SET search_path TO {schema_name};\"))\n",
    "def export_table_data(query, params, connections, common_dump_file_name, schema_name, table_name, export_format='text', export_path='/app/outputs', log_file_name='query_log.txt',tables_file='tables_log.txt', CHUNK_SIZE=10000):\n",
    "    if not os.path.exists(export_path):\n",
    "        os.makedirs(export_path)\n",
    "\n",
    "    log_file_path = os.path.join(export_path, log_file_name)\n",
    "    dump_file_path = os.path.join(export_path, f\"{common_dump_file_name}.csv\")\n",
    "    tables_file_path = os.path.join(export_path, tables_file)\n",
    "\n",
    "   \n",
    "    with open(log_file_path, 'a', encoding='utf-8') as log_file:\n",
    "        # Convert the query object to a string\n",
    "        query_str = str(query)\n",
    "        param_value = repr(params['key_value']) if isinstance(params['key_value'], str) else str(params['key_value'])\n",
    "        query_str_with_params = query_str.replace(\":key_value\", param_value)\n",
    "        # Write the modified query to the log file\n",
    "        log_file.write(f\"Table: {table_name}\\n{query_str_with_params}\\n\")\n",
    "\n",
    "    with open(tables_file_path, 'a', encoding='utf-8') as tables_file:\n",
    "        # Write the modified query to the log file\n",
    "        tables_file.write(f\"Table: {table_name}\\n\")\n",
    "  \n",
    "    with engine.connect() as connection:\n",
    "        try:\n",
    "            first_chunk = True\n",
    "            print(f\"running for table -> {table_name} chunks with params {params} ----the query {query}\")\n",
    "            for chunk in pd.read_sql_query(query, connection, params=params, chunksize=CHUNK_SIZE):\n",
    "                if first_chunk:\n",
    "                    # Write column names and COPY command only for the first chunk\n",
    "                    column_names = '\\t '.join(chunk.columns)\n",
    "                    header_str = f\"{schema_name}.{table_name}\\n{column_names}\\n\"  # Removed the closing parenthesis\n",
    "                    first_chunk = False\n",
    "                else:\n",
    "                    header_str = ''\n",
    "\n",
    "                chunk.dropna(how='all', inplace=True)\n",
    "\n",
    "                # Convert the DataFrame to a CSV formatted string\n",
    "                csv_string = chunk.to_csv(sep='\\t', index=False, header=False)\n",
    "\n",
    "                # Append the header and CSV string to the dump file\n",
    "                with open(dump_file_path, 'a', encoding='utf-8') as dump_file:\n",
    "                    dump_file.write(header_str + csv_string)\n",
    "\n",
    "            # # Write the end-of-data marker\n",
    "            with open(dump_file_path, 'a', encoding='utf-8') as dump_file:\n",
    "                dump_file.write(\"\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    print(f\"Data appended to {dump_file_path} with query logged in {log_file_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "def generate_truncate_sql(export_path, related_tables):\n",
    "    truncate_file_path = os.path.join(export_path, \"truncate_tables.sql\")\n",
    "    with open(truncate_file_path, 'w', encoding='utf-8') as truncate_file:\n",
    "        for table in related_tables:\n",
    "            truncate_file.write(f\"TRUNCATE TABLE {schema_name}.{table} restart identity cascade;\\n\")\n",
    "    print(f\"Truncate script created at {truncate_file_path}\")\n",
    "\n",
    "\n",
    "def get_key_value(table, column, value, connection):\n",
    "    \"\"\"\n",
    "    Fetch the corresponding key value from a table based on a column and its value.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT {column} FROM {table} WHERE {column} = %s;\"\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(query, [value])\n",
    "        result = cur.fetchone()\n",
    "        return result[0] if result else None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Usage example\n",
    "common_dump_file_name = \"brands_dump\"\n",
    "# Retrieve script parameters from environment variables\n",
    "\n",
    "\n",
    "def get_related_tables2(engine, table_name, excluded_tables, excluded_columns):\n",
    "    related_tables = []\n",
    "    \n",
    "    foreign_key_query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        tc.table_name AS primary_table, \n",
    "        kcu.column_name AS primary_column, \n",
    "        ccu.table_name AS foreign_table_name,\n",
    "        ccu.column_name AS foreign_column\n",
    "    FROM \n",
    "        information_schema.table_constraints AS tc \n",
    "        JOIN information_schema.key_column_usage AS kcu \n",
    "        ON tc.constraint_name = kcu.constraint_name\n",
    "        JOIN information_schema.constraint_column_usage AS ccu \n",
    "        ON ccu.constraint_name = tc.constraint_name\n",
    "    WHERE \n",
    "        tc.constraint_type = 'FOREIGN KEY' AND \n",
    "        (tc.table_name = :current_table OR ccu.table_name = :current_table);\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(sqlalchemy.text(foreign_key_query), {'current_table': table_name})\n",
    "        rows = result.fetchall()\n",
    "\n",
    "        for row in rows:\n",
    "            primary_table, primary_column, foreign_table_name, foreign_column = row\n",
    "            if primary_column in excluded_columns or foreign_column in excluded_columns:\n",
    "                continue\n",
    "\n",
    "            new_table = foreign_table_name if primary_table == table_name else primary_table\n",
    "            if new_table not in excluded_tables:\n",
    "                related_tables.append(new_table)\n",
    "\n",
    "    return list(set(related_tables))\n",
    "\n",
    "def bfs_traverse(engine, start_table, schema_name, key_column, key_value, bfs_output_file, excluded_tables, excluded_columns):\n",
    "    \n",
    "    if excluded_tables is None:\n",
    "        excluded_tables = set()\n",
    "    if excluded_columns is None:\n",
    "        excluded_columns = set()\n",
    "    \n",
    "    queue = deque([(start_table, [])])\n",
    "    paths = {}\n",
    "    visited = set()\n",
    "\n",
    "    with open(bfs_output_file, 'w') as file:\n",
    "        while queue:\n",
    "            current_table, path = queue.popleft()\n",
    "            path_signature = tuple(path + [current_table])\n",
    "\n",
    "            if path_signature in visited or current_table in excluded_tables:\n",
    "                continue\n",
    "            visited.add(path_signature)\n",
    "\n",
    "            if current_table not in paths:\n",
    "                paths[current_table] = []\n",
    "            paths[current_table].append(path + [current_table])\n",
    "\n",
    "            for related_table in get_related_tables2(engine, current_table, excluded_tables, excluded_columns):\n",
    "                if related_table not in path:\n",
    "                    queue.append((related_table, path + [current_table]))\n",
    "\n",
    "            # Write paths to file for debugging\n",
    "            file.write(f\"Table: {current_table}, Paths: {paths[current_table]}\\n\")\n",
    "\n",
    "    return paths\n",
    "\n",
    "\n",
    "def combine_union_queries(engine, start_table, start_table_key_column, key_value, schema_name, bfs_output_file, excluded_tables=None, excluded_columns=None):\n",
    "    paths = bfs_traverse(engine, start_table, schema_name, start_table_key_column, key_value, bfs_output_file, excluded_tables, excluded_columns)\n",
    "\n",
    "    for table, table_paths in paths.items():\n",
    "        if table == start_table:\n",
    "            continue\n",
    "\n",
    "        union_queries = []\n",
    "        for path in table_paths:\n",
    "            join_conditions = ' '.join([f'LEFT JOIN \"{schema_name}\".\"{p}\" ON ...' for p in path[1:]])\n",
    "            query = f'SELECT * FROM \"{schema_name}\".\"{start_table}\" {join_conditions} WHERE \"{schema_name}\".\"{start_table}\".\"{start_table_key_column}\" = {key_value}'\n",
    "            union_queries.append(query)\n",
    "\n",
    "        final_query = ' UNION '.join(union_queries)\n",
    "        with engine.connect() as conn:\n",
    "            export_table_data(final_query, {'key_value': key_value}, conn, common_dump_file_name, schema_name, table, export_format='text')\n",
    "\n",
    "        #print(f\"Table: {table}, Query: {final_query}\\n\")\n",
    "\n",
    "def find_related_tables_bfs_enhanced2(engine, starting_table, start_table_key_column, key_value, common_dump_file_name, schema_name, bfs_output_file, excluded_tables=None, excluded_columns=None):\n",
    "    if excluded_tables is None:\n",
    "        excluded_tables = set()\n",
    "    if excluded_columns is None:\n",
    "        excluded_columns = {'modified_by_id', 'created_by_id', 'symbol_id'}\n",
    "\n",
    "    queue = deque([(starting_table, [starting_table], [])])  # Initialize with current_table, path, and join conditions\n",
    "    paths_to_table = {}\n",
    "    visited = set()\n",
    "\n",
    "    with open(bfs_output_file, 'w') as file:\n",
    "        while queue:\n",
    "            current_table, path, joins = queue.popleft()\n",
    "            path_signature = (current_table, tuple(path))\n",
    "\n",
    "            if path_signature in visited:\n",
    "                continue\n",
    "            visited.add(path_signature)\n",
    "\n",
    "            if current_table not in paths_to_table:\n",
    "                paths_to_table[current_table] = []\n",
    "            paths_to_table[current_table].append((path, joins))\n",
    "\n",
    "            foreign_key_query = \"\"\"\n",
    "            SELECT DISTINCT\n",
    "                tc.table_name AS primary_table, \n",
    "                kcu.column_name AS primary_column, \n",
    "                ccu.table_name AS foreign_table_name,\n",
    "                ccu.column_name AS foreign_column\n",
    "            FROM \n",
    "                information_schema.table_constraints AS tc \n",
    "                JOIN information_schema.key_column_usage AS kcu \n",
    "                ON tc.constraint_name = kcu.constraint_name\n",
    "                JOIN information_schema.constraint_column_usage AS ccu \n",
    "                ON ccu.constraint_name = tc.constraint_name\n",
    "            WHERE \n",
    "                tc.constraint_type = 'FOREIGN KEY' AND \n",
    "                (tc.table_name = :current_table OR ccu.table_name = :current_table);\n",
    "            \"\"\"\n",
    "\n",
    "            with engine.connect() as conn:\n",
    "                result = conn.execute(sqlalchemy.text(foreign_key_query), {'current_table': current_table})\n",
    "                rows = result.fetchall()\n",
    "\n",
    "                for row in rows:\n",
    "                    primary_table, primary_column, foreign_table_name, foreign_column = row\n",
    "\n",
    "                    if primary_column in excluded_columns or foreign_column in excluded_columns:\n",
    "                        continue\n",
    "\n",
    "                    if primary_table != current_table and primary_table not in excluded_tables:\n",
    "                        new_table = primary_table\n",
    "                        new_join_condition = f'LEFT JOIN \\\"{schema_name}\\\".\\\"{primary_table}\\\" ON \\\"{schema_name}\\\".\\\"{current_table}\\\".\\\"{primary_column}\\\" = \\\"{schema_name}\\\".\\\"{primary_table}\\\".\\\"{foreign_column}\\\"'\n",
    "                    elif foreign_table_name != current_table and foreign_table_name not in excluded_tables:\n",
    "                        new_table = foreign_table_name\n",
    "                        new_join_condition = f'LEFT JOIN \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\" ON \\\"{schema_name}\\\".\\\"{current_table}\\\".\\\"{foreign_column}\\\" = \\\"{schema_name}\\\".\\\"{foreign_table_name}\\\".\\\"{primary_column}\\\"'\n",
    "                    else:\n",
    "                        continue  # Skip if the new table is the current one or in excluded tables\n",
    "\n",
    "                    if new_table not in path:  # Check if the new table is not already in the current path\n",
    "                        new_path = path + [new_table]\n",
    "                        new_joins = joins + [new_join_condition]\n",
    "                        new_path_signature = (new_table, tuple(new_path))\n",
    "\n",
    "                        if new_path_signature not in visited:\n",
    "                            queue.append((new_table, new_path, new_joins))\n",
    "\n",
    "                            # Writing to the file\n",
    "                            file.write(f\"New Table: {new_table}, New Path: {' -> '.join(new_path)}, New Joins: {' '.join(new_joins)}\\n\")\n",
    "\n",
    "    # Combine queries for each table using UNION and export data\n",
    "    for table, paths in paths_to_table.items():\n",
    "        combined_query_parts = []\n",
    "        for path, joins in paths:\n",
    "            join_conditions = ' '.join(joins)\n",
    "            query = f'SELECT DISTINCT \"{schema_name}\".\"{table}\".* FROM \"{schema_name}\".\"{starting_table}\" {join_conditions} WHERE \"{start_table_key_column}\" = :key_value'\n",
    "            combined_query_parts.append(query)\n",
    "\n",
    "        combined_query = ' UNION '.join(combined_query_parts) if len(combined_query_parts) > 1 else combined_query_parts[0]\n",
    "        with engine.connect() as conn:\n",
    "            export_table_data(combined_query, {'key_value': key_value}, conn, common_dump_file_name, schema_name, table, export_format='text')\n",
    "\n",
    "db_params = {\n",
    "    'host': os.getenv('DB_HOST', 'default_host'),\n",
    "    'dbname': os.getenv('DB_NAME', 'default_db_name'),\n",
    "    'user': os.getenv('DB_USER', 'default_user'),\n",
    "    'password': os.getenv('DB_PASSWORD', 'default_password'),\n",
    "    'port': int(os.getenv('DB_PORT', '5432'))  # Default port number if not specified\n",
    "}\n",
    "\n",
    "\n",
    "starting_table = os.getenv('STARTING_TABLE', 'default_value_for_starting_table')\n",
    "starting_key_column = os.getenv('STARTING_KEY_COLUMN', 'default_value_for_starting_key_column')\n",
    "\n",
    "# For 'key_value', handle different data types (string/int)\n",
    "key_value_str = os.getenv('KEY_VALUE', 'default_value_for_key_value')\n",
    "try:\n",
    "    key_value = int(key_value_str)\n",
    "except ValueError:\n",
    "    key_value = key_value_str  # Use the string value if it's not an integer\n",
    "\n",
    "# Split the excluded tables into a list if they are provided as a comma-separated string\n",
    "excluded_tables_str = os.getenv('EXCLUDED_TABLES', '')\n",
    "excluded_tables = excluded_tables_str.split(',') if excluded_tables_str else []\n",
    "excluded_columns = {'modified_by_id', 'created_by_id', 'symbol_id'}\n",
    "# Global chunk size for data processing\n",
    "CHUNK_SIZE = 10000  # You can adjust this value as needed\n",
    "#print(\"Hierarchy starting from '{}', for key '{}': {}\".format(starting_table, starting_key_column, key_value))\n",
    "#related_tables = find_related_tables_bfs_enhanced2(engine,starting_table, starting_key_column, key_value, common_dump_file_name, schema_name,'/app/outputs/debuging.txt', excluded_tables=excluded_tables)\n",
    "#def find_related_tables_bfs(engine, starting_table, start_table_key_column, key_value, common_dump_file_name, schema_name, excluded_tables=None, excluded_columns=None):\n",
    "\n",
    "#combine_union_queries(engine, starting_table, schema_name, starting_key_column)  \n",
    "\n",
    "#print(f\"Related tables: {related_tables}\")\n",
    "\n",
    "combine_union_queries(engine, starting_table, starting_key_column, key_value, schema_name, '/app/outputs/bfs_output.txt', excluded_tables, excluded_columns)\n",
    "\n",
    "# Specify the file path where you want to save the output\n",
    "output_file_path = '/app/exports/related_tables_output.txt'\n",
    "\n",
    "# # Writing the related tables to a file\n",
    "# with open(output_file_path, 'w') as file:\n",
    "#     for table in related_tables:\n",
    "#         file.write(f\"{table}\\n\")\n",
    "\n",
    "# print(f\"Related tables have been written to {output_file_path}\")\n",
    "\n",
    "#generate_truncate_sql('/app/exports/', related_tables)\n",
    "#engine.dispose()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
